# papers
読んだ論文についていろいろまとめる

# 読んだもの
1. [Survey on Recurrent Neural Network in Natural Language Processing](https://www.researchgate.net/publication/319937209_Survey_on_Recurrent_Neural_Network_in_Natural_Language_Processing)
1. [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)
1. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
1. [Language Through a Prism: A Spectral Approach for Multiscale Language Representations](https://proceedings.neurips.cc/paper/2020/file/3acb2a202ae4bea8840224e6fce16fd0-Paper.pdf)
1. [Meta-learning for Few-shot Natural Language Processing: A Survey](https://arxiv.org/abs/2007.09604)
1. [Generalizing from a Few Examples: A Survey on Few-Shot Learning](https://arxiv.org/abs/1904.05046)
1. [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
1. [Language Models are Unsupervised Multitask Learners(GPT-2)](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

# 読もうと思っているもの
1. [Language Models are Few-Shot Learners (GPT-3)](https://arxiv.org/abs/2005.14165)
1. [Order-Embeddings of Images and Language](https://arxiv.org/abs/1511.06361)
1. [Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models](https://arxiv.org/abs/1411.2539)
1. [Deep Contextualized Word Representations (ELMo)](https://www.aclweb.org/anthology/N18-1202/)
1. [A Primer in BERTology: What we know about how BERT works](https://arxiv.org/abs/2002.12327)
1. [Reinforcement Learning: A Survey](https://www.jair.org/index.php/jair/article/view/10166)
1. [Improving Language Understanding by Generative Pre-Training(GPT-1)](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)
1. [Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks (MAML)](https://arxiv.org/abs/1703.03400)
