\documentclass{jsarticle}
\bibliographystyle{junsrt}
\usepackage[dvipdfmx]{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{wrapfig}
\usepackage{MnSymbol}
\usepackage{scalefnt}
\usepackage{here}
\title{\vspace{-3cm}パターン認識\\レポート課題}
\author{井上智裕}
\date{2021年1月16日}

\makeatletter
\renewcommand{\thefigure}{\thesection.\arabic{figure}}
\@addtoreset{figure}{section}
\renewcommand{\thetable}{\thesection.\arabic{table}}
\@addtoreset{table}{section}
\renewcommand{\theequation}{\thesection.\arabic{equation}}
\@addtoreset{equation}{section}

\newcommand{\argmax}{\mathop{\rm arg~max}\limits}

\begin{document}
\maketitle
\vspace{-1cm}
\section{選んだ論文}
% 論文著者名，論文名，国際会議名または論文誌名，論文誌の場合巻・号，ページ，発
% 行年，論文 URL 等の情報（論文を理解するために参照した他の書籍・論文・web ペー
% ジ等あればそれも記述する）
Alex Tamkin, Dan Jurafsky, and Noah Goodman. Language through a prism: A spectral approach for multiscale language representations, NeurIPS, 2020, https://arxiv.org/abs/2011.04823

\section{導入}
% 問題設定の概要（もしあれば具体的な応用問題，および，識別・次元削減・クラスタ
% リング等データ解析の目的を適宜数式等を用いて記述）．できるだけ self-consistent
% であること (つまり，記号はすべてきちんと定義し，予備知識のない非専門家がそれ
% だけを読んで理解できるよう工夫すること．そのためには別に参考書や論文を読む
% 必要がある場合もある．読んだ参考書や論文の書誌情報も記載すること．)
\subsection{扱う問題}
% 画像処理か？音声認識か？
自然言語処理分野。
特定の言語モデルに頼らない、自然言語におけるスケールの異なる構造（語彙、節、文書など）の
発見・学習。

\subsection{問題意識}
% どこに問題意識を感じているのか？既存手法では何が足りないのか？
単語の意味など語彙レベルの構造、節や文レベルでの構造、
文書全体の主題構造や物語構造など、言語は異なるレベルでの構造を有している。
従来の研究では、単語の分散ベクトル表現の学習\cite{mikolov2013distributed}や
文の分散表現の学習\cite{hill-etal-2016-learning-distributed}といった個々のレベルでの構造を捉えるためのモデルの構築など、
異なる階層の言語構造のそれぞれを明示的にモデル化する方法が示されてきた。
この論文では、従来手法のように文や節といった
特定の構造レベルの言語モデルに頼ることなく、
あらゆるスケールの表現において、各レベルでの構造を発見・学習するための
方法を提案している。

\section{理論}
% 手法とその特徴
% 使われているデータ解析の手法 (アルゴリズムや，新規手法であればその導出のポイ
% ント．理論であれば解析手法など) これも適宜数式等を用いて記述する
% その手法の特徴 (長所・短所など)
% \section{理論}
% どのようなアイデア・ロジック・仮定で問題を解決しようとしているか？なぜそれで問題が解決できるのか？
% \subsection{定式化・アルゴリズム}
% アイデアをどのように定式に落とし込んでいるか？それぞれの式は何を意味しているのか？
言語における異なるレベルの構造の発見にあたり、
信号処理分野などで広く使われているスペクトル分析の手法を導入する。
入力系列に対し周波数領域で演算を行うためには、入力表現を周波数領域での表現に変換する必要がある。
このスペクトル変換には、離散コサイン変換（DCT）を使用する。
実数列$\{x^{(0)},\cdots, x^{(N-1)}\}$に対して、DCT（各周波数の重み）は次式で得られる。
\begin{eqnarray}
  f^{(k)}= \sum_{n=0}^{N-1}x^{(n)}\cos{[\frac{\pi}{N}(n+\frac{1}{2})k]} \ \ \ \ k=0,\cdots,N-1
\end{eqnarray}
このDCTを言語表現に適用するにあたり、
文脈に応じた単語表現を対象とする。
文脈に応じた単語表現とは、トークン（単語やサブワード単位）の入力系列に対し、
自然言語処理モデルが処理することで生成されるベクトル系列である。
これは最近の自然言語処理モデルに共通する特徴量であり、
前述の自然言語処理モデルにはBERT\cite{devlin2019bert}や
GPT-2\cite{radford2018gpt2}のような
Transformer\cite{vaswani2017attention}ベースのモデルや
ELMo\cite{peters-etal-2018-deep}のような
LSTM\cite{hochreiter1997long}ベースのモデルを含む。
文脈に応じた単語表現$v_0,\cdots,v_{N-1}$が与えられたとき、
単語表現のあるニューロン（次元）$i$に沿った表現$v_0[i],\cdots,v_{N-1}[i]$にDCTを適用し、
得られる系列$f_0[i],\cdots,f_{N-1}[i]$を第$i$ニューロンのスペクトルとする。

この手法では、ニューロンのスペクトルに対して、特定の閾値を持つスペクトルフィルタを通すことによる
特定成分の除去と
逆離散コサイン変換（IDCT）による元のドメインへの変換を行うことにより、
系列から特定のスケールの構造を取り出すことができる。
この研究では、入力サイズを512として、スケールに応じて表\ref{tab:tab1}に示す5種類のフィルタを用意した。

\begin{table}[H]
  \begin{center}
    \caption{スペクトルフィルタ}
    \begin{tabular}{cccc} \hline
      フィルタ  & 対応する言語構造 & 周期[tokens] & DCT index $k$  \\ \hline 
      HIGH     & 単語 & $1-2$        & $130 - 511$    \\
      MID-HIGH & 節   & $2-8$        & $34 - 129$  \\
      MID      & 文   & $8-32$       & $9 - 33$  \\ 
      MI-LOW   & 段落 & $32-256$     & $2 - 8$ \\
      LOW      & 文書 & $256-\infty$ & $0 - 1$ 
      \\ \hline
    \end{tabular}
    \label{tab:tab1}
  \end{center}
\end{table}

\section{実験}
% どんな実験設定か？どんな点で他の手法より良くなったか？
スペクトルフィルタにより、異なるスケールの言語構造が取り出せるか評価を行うための
実験を行う。
スペクトルフィルタによりフィルタリングされた表現を用いて
異なるスケールのタスクを行った時、スペクトルフィルタの選択が分類器の
能力にどのように影響するか比較する。
以下の各データセットに対して、$BERT_{BASE}$モデル\cite{devlin2019bert}を用いて
768次元の単語表現を得る。
次に、各次元に沿ってスペクトルフィルタを適用し、フィルタリングされた表現を
用いて特定タスクを実行するソフトマックス分類器を学習させる。
用いたデータセット、タスクは以下の通りである。

\begin{enumerate}
  \item 文章タグ付け（単語レベル）。Penn Treebankデータセット
  を使用する。タスクは、与えられたトークン表現から品詞（例：動詞過去形、wh代名詞、数詞）を予測することである。
  \item 対話の分類（発話レベル）。Switchboard Dialog Speech Acts コーパスを使用する。
  このタスクは、与えられたトークン表現を含む発話の対話の分類
  （例：謝罪、言葉を濁す、感謝）を予測することである。
  \item トピック分類（文書レベル）。20 Newsgroups データセット を使用する。
  与えられたトークン表現を含む文書のトピック
  （ニュースグループ；例：SCI.SPACE、COMP.GRAPHICS、REC.AUTOS）を予測する。
  \item masked language modeling (MLM) タスク。
  単語の抜けがある文章の抜けを予測するタスク。
  BERTの事前学習で利用されるため、スペクトルフィルタ適用前の表現のターゲットタスクである。
\end{enumerate}

各タスクの実行結果を図\ref{fig:fig1}に示す。
図\ref{fig:fig1}より、単語レベルのタスクである文章タグ付けではHIGHバンドを用いた
ときの精度が最も高い一方、フィルタリング前より性能が下がっている。
これは、単語レベルの情報を主に使う一方で低周波数情報も必要としていることを示す。
一方、文書レベルのタスクであるトピック分類タスクでは
LOWバンドを用いたときの精度が最も高く、
元の表現よりも高い性能を示した。
これは、元の表現に存在する高い周波数変動がこのタスクにおいては、
精度に悪影響を及ぼしている可能性があることを示している。
また、発話を対象とした対話の分類タスクでは、
MIDフィルタを用いたときの精度がもっとも高い性能を示した。
また、MLMタスクの結果は文章タグ付けの結果に最も類似しており、
MLMタスクが局所的なタスクであることを確認できた。

\begin{figure}[H]
  \begin{center}
    \includegraphics[clip,width=12.0cm]{image/04_Prism/fig1.PNG}
    \caption{各タスクの実行結果}
    \label{fig:fig1}
  \end{center}
\end{figure}

\section{結論}
% データ解析を行って得られた結論（理論の場合はその主結果）

\section{論文に対する考察}
% 論文を読んで自分で考察したこと，感じたこと（さらに改良できそうなアイディア
% や，同じ問題に対して考えられるよりよいアプローチなど自分で研究プランを立て
% るつもりで）
ネットワーク全体ではなく、ニューロン単位で時系列の周波数情報を見ているのが面白い。





\section{結論・展望}
他に改善するとしたらどこか？

%% \begin{table}[htbp]
%%   \begin{center}
%%     \caption{何かの表}
%%     \begin{tabular}{|c|c|c|c|c|} \hline
%%       ユーザー & 映画1 & 映画2 & 映画3 & 映画4 \\ \hline 
%%       A & 5 & 3 &   &  \\
%%       B &  & 2 &  & 5 \\
%%       C &  &  & 4 & 3 \\ 
%%       D & 4 &  & 3 & \\
%%       E & 4 & 3 &  & 2\\ \hline
%%     \end{tabular}
%%     \label{tab:tab1}
%%   \end{center}
%% \end{table}

%% \begin{figure}[htbp]
%%   \begin{center}
%%     \includegraphics[clip,width=12.0cm]{/path/to/hoge.png}
%%     \caption{何かの図}
%%     \label{fig:fig1}
%%   \end{center}
%% \end{figure}

% \begin{figure}[H]
%   \begin{center}
%     \includegraphics[clip,width=12.0cm]{image/02_Attention/fig1.PNG}
%     \caption{NMT}
%     \label{fig:fig1}
%   \end{center}
% \end{figure}

% \begin{eqnarray}
%   score(h_t,\bar h_s) = \begin{cases}
%     h_t^T\bar h_s & dot \\
%     h_t^TW_a\bar h_s & general \\
%     v_a^T \tanh(W_a [h_t;\bar h_s]) & concat \\
%   \end{cases}
% \end{eqnarray}

\bibliography{ref}
\end{document}
