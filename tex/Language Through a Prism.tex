\documentclass{jsarticle}
\bibliographystyle{junsrt}
\usepackage[dvipdfmx]{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{wrapfig}
\usepackage{MnSymbol}
\usepackage{scalefnt}
\usepackage{here}
\title{\vspace{-3cm}パターン認識\\レポート課題}
\author{井上智裕}
\date{2021年1月16日}

\makeatletter
\renewcommand{\thefigure}{\thesection.\arabic{figure}}
\@addtoreset{figure}{section}
\renewcommand{\thetable}{\thesection.\arabic{table}}
\@addtoreset{table}{section}
\renewcommand{\theequation}{\thesection.\arabic{equation}}
\@addtoreset{equation}{section}

\newcommand{\argmax}{\mathop{\rm arg~max}\limits}

\begin{document}
\maketitle
\vspace{-1cm}
\section{選んだ論文}
% 論文著者名，論文名，国際会議名または論文誌名，論文誌の場合巻・号，ページ，発
% 行年，論文 URL 等の情報（論文を理解するために参照した他の書籍・論文・web ペー
% ジ等あればそれも記述する）
Alex Tamkin, Dan Jurafsky, and Noah Goodman. Language through a prism: A spectral approach for multiscale language representations, NeurIPS, 2020, https://arxiv.org/abs/2011.04823

\section{導入}
% 問題設定の概要（もしあれば具体的な応用問題，および，識別・次元削減・クラスタ
% リング等データ解析の目的を適宜数式等を用いて記述）．できるだけ self-consistent
% であること (つまり，記号はすべてきちんと定義し，予備知識のない非専門家がそれ
% だけを読んで理解できるよう工夫すること．そのためには別に参考書や論文を読む
% 必要がある場合もある．読んだ参考書や論文の書誌情報も記載すること．)
\subsection{扱う問題}
% 画像処理か？音声認識か？
自然言語処理分野。
特定の言語モデルに頼らない、自然言語におけるスケールの異なる構造（語彙、節、文書など）の
発見・学習。

\subsection{問題意識}
% どこに問題意識を感じているのか？既存手法では何が足りないのか？
単語の意味など語彙レベルの構造、節や文レベルでの構造、
文書全体の主題構造や物語構造など、言語は異なるレベルでの構造を有している。
従来の研究では、単語の分散ベクトル表現の学習\cite{mikolov2013distributed}や
文の分散表現の学習\cite{hill-etal-2016-learning-distributed}といった
個々のレベルでの構造を捉えるためのモデルの構築など、
異なる階層の言語構造のそれぞれを明示的にモデル化する方法が示されてきた。
そんな中、近年では様々なタスクに対応するための汎用事前学習モデルが作られるようになったが、
これらのモデルでは特定の階層の情報を特定のニューロンが担うのではなく、
ニューロン全体に情報が分散している。
この論文では、従来手法のように文や節といった
特定の構造レベルの言語モデルに頼ることなく、
BERT\cite{devlin2019bert}のような汎用モデルから
各スケールの構造を抽出・学習するための
方法を提案している。

\section{理論}
% 手法とその特徴
% 使われているデータ解析の手法 (アルゴリズムや，新規手法であればその導出のポイ
% ント．理論であれば解析手法など) これも適宜数式等を用いて記述する
% その手法の特徴 (長所・短所など)
% \section{理論}
% どのようなアイデア・ロジック・仮定で問題を解決しようとしているか？なぜそれで問題が解決できるのか？
% \subsection{定式化・アルゴリズム}
% アイデアをどのように定式に落とし込んでいるか？それぞれの式は何を意味しているのか？
\subsection{スペクトルフィルタ}
言語における異なるレベルの構造の発見にあたり、
信号処理分野などで広く使われているスペクトル分析の手法を導入する。
入力系列に対し周波数領域で演算を行うためには、入力表現を周波数領域での表現に変換する必要がある。
このスペクトル変換には、離散コサイン変換（DCT）を使用する。
実数列$\{x^{(0)},\cdots, x^{(N-1)}\}$に対して、DCT（各周波数の重み）は次式で得られる。
\begin{eqnarray}
  f^{(k)}= \sum_{n=0}^{N-1}x^{(n)}\cos{[\frac{\pi}{N}(n+\frac{1}{2})k]} \ \ \ \ k=0,\cdots,N-1
\end{eqnarray}
このDCTを言語表現に適用するにあたり、
文脈に応じた単語表現を対象とする。
文脈に応じた単語表現とは、トークン（単語やサブワード単位）の入力系列に対し、
自然言語処理モデルが各入力の前後の文脈を考慮に加えて処理することで生成されるベクトル系列である。
% これは最近の自然言語処理モデルに共通する特徴量であり、
% 前述の自然言語処理モデルにはBERT\cite{devlin2019bert}や
% GPT-2\cite{radford2018gpt2}のような
% Transformer\cite{vaswani2017attention}ベースのモデルや
% ELMo\cite{peters-etal-2018-deep}のような
% LSTM\cite{hochreiter1997long}ベースのモデルを含む。
文脈に応じた単語表現$v_0,\cdots,v_{N-1}$が与えられたとき、
単語表現のあるニューロン（次元）$i$に沿った表現$v_0[i],\cdots,v_{N-1}[i]$にDCTを適用し、
得られる系列$f_0[i],\cdots,f_{N-1}[i]$を第$i$ニューロンのスペクトルとする。

この手法では、ニューロンのスペクトルに対して、特定の閾値を持つスペクトルフィルタを通すことによる
特定成分の除去と
逆離散コサイン変換（IDCT）による元のドメインへの変換を行うことにより、
系列から特定のスケールの構造を取り出すことができる。
この研究では、入力サイズを512として、スケールに応じて表\ref{tab:tab1}に示す5種類のフィルタを用意した。

\begin{table}[H]
  \begin{center}
    \caption{スペクトルフィルタ}
    \begin{tabular}{cccc} \hline
      フィルタ  & 対応する言語構造 & 周期[tokens] & DCT index $k$  \\ \hline 
      HIGH     & 単語 & $1-2$        & $130 - 511$    \\
      MID-HIGH & 節   & $2-8$        & $34 - 129$  \\
      MID      & 文   & $8-32$       & $9 - 33$  \\ 
      MI-LOW   & 段落 & $32-256$     & $2 - 8$ \\
      LOW      & 文書 & $256-\infty$ & $0 - 1$ 
      \\ \hline
    \end{tabular}
    \label{tab:tab1}
  \end{center}
\end{table}

\subsection{プリズム層}
BERT\cite{devlin2019bert}は近年提案された自然言語処理分野における
汎用事前学習済みモデルである。
BERTは、ランダムにマスクされた入力系列のマスクされた部分を予測するタスクである
masked language modeling (MLM) タスクと、
ある文Aに対して別のある文Bが実際に続く文か予測するタスクである
next sentence prediction (NSP) タスクによって事前学習が行われる。
これによって、BERTは前後の文脈を考慮した単語レベルでの表現学習や
文同士の関係性の学習を行うことができるため、
様々な階層のタスクに適応できる事前学習済みモデルとして利用できる。
しかしながら、この事前学習の方法では
特定のニューロンが特定の階層のタスクに特化するのではなく、
各階層のタスクに関する情報がすべてのニューロンに分散している可能性があるといえる。
そこで、この研究では学習においてスペクトルフィルタを利用することで、
異なるスケールのタスクに対して異なるニューロンを使用する方法を提案している。
BERTのユニットを5等分し、それぞれに表\ref{tab:tab1}の5種類のフィルタを対応させ、適用する。
この操作をプリズム層として最後のBERT層のあとに加えることにする。
このプリズム層を加えた事前学習済みBERTモデルをMLMタスクを用いて学習させる。

\section{実験}
% どんな実験設定か？どんな点で他の手法より良くなったか？
\subsection{言語表現に対するスペクトルフィルタの適用}
スペクトルフィルタにより、異なるスケールの言語構造が取り出せるか評価を行うための
実験を行う。
スペクトルフィルタによりフィルタリングされた表現を用いて
異なるスケールのタスクを行った時、スペクトルフィルタの選択が分類器の
能力にどのように影響するか比較する。
以下の各データセットに対して、$BERT_{BASE}$モデル\cite{devlin2019bert}を用いて
768次元の単語表現を得る。
次に、各次元に沿ってスペクトルフィルタを適用し、フィルタリングされた表現を
用いて特定タスクを実行するソフトマックス分類器を学習させる。
用いたデータセット、タスクは以下の通りである。

\begin{enumerate}
  \item 文章タグ付け（単語レベル）。Penn Treebankデータセット
  を使用する。タスクは、与えられたトークン表現から品詞（例：動詞過去形、wh代名詞、数詞）を予測することである。
  \item 対話の分類（発話レベル）。Switchboard Dialog Speech Acts コーパスを使用する。
  このタスクは、与えられたトークン表現を含む発話の対話の分類
  （例：謝罪、言葉を濁す、感謝）を予測することである。
  \item トピック分類（文書レベル）。20 Newsgroups データセット を使用する。
  与えられたトークン表現を含む文書のトピック
  （ニュースグループ；例：SCI.SPACE、COMP.GRAPHICS、REC.AUTOS）を予測する。
  \item masked language modeling (MLM) タスク。
  単語の抜けがある文章の抜けを予測するタスク。
  BERTの事前学習で利用されるため、スペクトルフィルタ適用前の表現のターゲットタスクである。
\end{enumerate}

各タスクの実行結果を図\ref{fig:fig1}に示す。
図\ref{fig:fig1}より、単語レベルのタスクである文章タグ付けではHIGHバンドを用いた
ときの精度が最も高い一方、フィルタリング前より性能が下がっている。
これは、単語レベルの情報を主に使う一方で低周波数情報も必要としていることを示す。
一方、文書レベルのタスクであるトピック分類タスクでは
LOWバンドを用いたときの精度が最も高く、
元の表現よりも高い性能を示した。
これは、元の表現に存在する高い周波数変動がこのタスクにおいては、
精度に悪影響を及ぼしている可能性があることを示している。
また、発話を対象とした対話の分類タスクでは、
MIDフィルタを用いたときの精度がもっとも高い性能を示した。
また、MLMタスクの結果は文章タグ付けの結果に最も類似しており、
MLMタスクが局所的なタスクであることを確認できた。

\begin{figure}[H]
  \begin{center}
    \includegraphics[clip,width=12.0cm]{image/04_Prism/fig1.PNG}
    \caption{各タスクの実行結果}
    \label{fig:fig1}
  \end{center}
\end{figure}

\subsection{プリズム層ありのモデルを用いた実験}
プリズム層を加えた事前学習済みBERTモデルをMLMタスクを用いて学習させる。
学習はWikiText-103データセットに対し、
Adamのデフォルトパラメータを用いて、
バッチサイズ8、5万ステップで学習を行う。
比較のために、プリズム層なしの事前学習済みのBERTモデルについても同様に学習を行う。
結果は表\ref{tab:tab2}に示すようにプリズム層を加えたモデルはそうでないモデルと比較して、
文章タグ付けのタスクで高い精度を維持しながら、トピック分類や対話の分類において大きく改善した。

\begin{table}[H]
  \begin{center}
    \caption{スペクトルフィルタ}
    \begin{tabular}{cccc} \hline
      タスク       & モデル           & 精度 (\%) & 標準偏差 (\%)  \\ \hline 
      トピック分類 & BERT             & 32.21     & 0.08    \\
                  & BERT+プリズム層   & 51.01     & 0.14  \\\hline 
      対話分類     & BERT             & 47.09     & 0.33  \\ 
                  & BERT+プリズム層   & 54.02     & 0.61 \\\hline 
      文章タグ付け & BERT             & 95.86     & 0.02 \\
                  & BERT+プリズム層   & 94.41     & 0.02 
      \\ \hline
    \end{tabular}
    \label{tab:tab2}
  \end{center}
\end{table}

また、入力の中央の連続した100個の単語がマスクされたMLMタスクにおける性能を比較したところ、
予測精度は図\ref{fig:fig2}のようになった。
これより、プリズム層がある場合はそうでない場合に比べて、
欠落したトークンを推測できる確率が高いことが分かる。
これより、BERT+プリズムモデルが遠くの文脈を考慮してトークンを予測するより優れた長距離言語モデルである
といえる。

\begin{figure}[H]
  \begin{center}
    \includegraphics[clip,width=12.0cm]{image/04_Prism/fig2.PNG}
    \caption{MLMタスクの実行結果}
    \label{fig:fig2}
  \end{center}
\end{figure}

\section{結論}
% データ解析を行って得られた結論（理論の場合はその主結果）
スペクトル分析の技術を自然言語処理モデルに導入することで
言語表現を言語の異なる階層ごとに適した表現に分離することが
できることが分かった。
また、プリズム層を用いた学習を行うことで
スケールの異なるタスクにおけるパフォーマンスを向上させたり、
長距離の文脈を考慮した推定の精度を向上させたりすることができた。

\section{論文に対する考察}
% 論文を読んで自分で考察したこと，感じたこと（さらに改良できそうなアイディア
% や，同じ問題に対して考えられるよりよいアプローチなど自分で研究プランを立て
% るつもりで）
この研究のユニークな点は、
時系列の入力に対して
深層学習モデルが生成した表現系列を信号処理的に
分析する点である。
表現系列を次元ごとに周波数領域に移すことで
利用される情報を時間軸のスケールごとに分けるアプローチは大変興味深く思った。
今回の分析では自然言語を対象としていたが、
この手法は他の領域であっても適用できる範囲が広いと思う。
例えば、動画は画像の時間的な連続データであるから、
動画に対してもスペクトルフィルタを用いることができる。
この場合、各画素の表現や畳み込みを行ったあとの情報を
プリズム層に通せば各領域の時間的な周期性を分析することができる。
あるいは、各画像に対しても空間的な周期を捉えるものとして
スペクトルフィルタの技術は利用できると考えられる。
こうした応用により、今回の研究でBERTを特定の階層のタスクに適応させたように、
既存の事前学習済モデルを特定の階層のタスクでパフォーマンスが上がるように
チューニングすることができると考えられる。

また、スペクトルフィルタの応用には、
モデルのパフォーマンスを改善できる可能性があるだけではなく、
あるタスクにはどの階層に着目したモデルが適切なのか考える
指針を与える可能性がある。
今回の研究では、3つのタスクを扱っていたが、
これにより、トピック分類では高周波情報が学習の妨げとなっていることや
文章タグ付けでは高周波情報だけではなく周期の大きい文脈情報も
必要となることが分かった。
このようにあるタスクに対する周波数ごとに分けたモデルの精度を比較することが
モデルを学習させる際に必要なデータの前処理や
モデルの構築に役立つ可能性がある。

また、今回の研究では最後の層のあとにプリズム層を加え、
スケールの異なる情報に特化したニューロンを作っていたが、
途中の層にフィルタリングを利用するという応用も考えられる。
例えば、途中の複数の層の後にフィルタを挟み、
層を経るごとに高い周波数からカットしていけば、
前の層ほど局所的な構造を見て、後の層ほど全体を見るというような
CNNに近いモデルを作ることができる可能性がある。
あるいは、途中にフィルタを加えたうえで
カットする周波数も学習するパラメタに加えて学習を行えば、
層やニューロンごとにタスクに適した周波数になるように学習させることができる
と考えられる。
これを行うことでタスクに対する精度の向上が見込めるだけではなく、
学習されたフィルタの周波数から
モデルがどのようにデータを分析しているか
可視化することができるという利点もある。

以上のように、スペクトルフィルタを導入することで
既存のモデルの改善のみならず、
タスクやモデルの分析にもつながるため、
研究の余地があると考えられる。



%% \begin{table}[htbp]
%%   \begin{center}
%%     \caption{何かの表}
%%     \begin{tabular}{|c|c|c|c|c|} \hline
%%       ユーザー & 映画1 & 映画2 & 映画3 & 映画4 \\ \hline 
%%       A & 5 & 3 &   &  \\
%%       B &  & 2 &  & 5 \\
%%       C &  &  & 4 & 3 \\ 
%%       D & 4 &  & 3 & \\
%%       E & 4 & 3 &  & 2\\ \hline
%%     \end{tabular}
%%     \label{tab:tab1}
%%   \end{center}
%% \end{table}

%% \begin{figure}[htbp]
%%   \begin{center}
%%     \includegraphics[clip,width=12.0cm]{/path/to/hoge.png}
%%     \caption{何かの図}
%%     \label{fig:fig1}
%%   \end{center}
%% \end{figure}

% \begin{figure}[H]
%   \begin{center}
%     \includegraphics[clip,width=12.0cm]{image/02_Attention/fig1.PNG}
%     \caption{NMT}
%     \label{fig:fig1}
%   \end{center}
% \end{figure}

% \begin{eqnarray}
%   score(h_t,\bar h_s) = \begin{cases}
%     h_t^T\bar h_s & dot \\
%     h_t^TW_a\bar h_s & general \\
%     v_a^T \tanh(W_a [h_t;\bar h_s]) & concat \\
%   \end{cases}
% \end{eqnarray}

\bibliography{ref}
\end{document}
