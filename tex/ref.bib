@misc{tamkin2020language,
      title={Language Through a Prism: A Spectral Approach for Multiscale Language Representations}, 
      author={Alex Tamkin and Dan Jurafsky and Noah Goodman},
      year={2020},
      eprint={2011.04823},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{hill-etal-2016-learning-distributed,
    title = "Learning Distributed Representations of Sentences from Unlabelled Data",
    author = "Hill, Felix  and
      Cho, Kyunghyun  and
      Korhonen, Anna",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N16-1162",
    doi = "10.18653/v1/N16-1162",
    pages = "1367--1377",
}

@misc{mikolov2013distributed,
      title={Distributed Representations of Words and Phrases and their Compositionality}, 
      author={Tomas Mikolov and Ilya Sutskever and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1310.4546},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{radford2018gpt2,
  added-at = {2019-02-27T03:35:25.000+0100},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  biburl = {https://www.bibsonomy.org/bibtex/2b30710316a8cfbae687672ea1f85c193/kirk86},
  description = {Language Models are Unsupervised Multitask Learners},
  interhash = {ce8168300081d74707849ed488e2a458},
  intrahash = {b30710316a8cfbae687672ea1f85c193},
  keywords = {learning multitask},
  timestamp = {2019-02-27T03:35:25.000+0100},
  title = {Language Models are Unsupervised Multitask Learners},
  url = {https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf},
  year = 2018
}

@misc{vaswani2017attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{peters-etal-2018-deep,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1202",
    doi = "10.18653/v1/N18-1202",
    pages = "2227--2237",
    abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@article{marcus-etal-1993-building,
    title = "Building a Large Annotated Corpus of {E}nglish: The {P}enn {T}reebank",
    author = "Marcus, Mitchell P.  and
      Santorini, Beatrice  and
      Marcinkiewicz, Mary Ann",
    journal = "Computational Linguistics",
    volume = "19",
    number = "2",
    year = "1993",
    url = "https://www.aclweb.org/anthology/J93-2004",
    pages = "313--330",
}

@misc{meta-gmo,
author = {GMO INTERNET GROUP},
title = {メタ学習（meta-learning）の紹介：Regression版で今年の東京の気温を当ててみました～ | GMOインターネット 次世代システム研究室},
year = {2018},
url = "https://recruit.gmo.jp/engineer/jisedai/blog/meta-learning/",
note = {(Accessed on 01/19/2021)}
}

@misc{meta-thomas,
author = {Thomas Wolf},
title = {From zero to research — An introduction to Meta-learning | by Thomas Wolf | HuggingFace | Medium},
url = {\url{https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a}},
month = {},
year = {},
note = {(Accessed on 01/19/2021)}
}

@misc{MetricLe5:online,
author = {},
title = {Metric Learning 入門 - copypasteの日記},
url = {\url{https://copypaste-ds.hatenablog.com/entry/2019/03/01/164155}},
month = {},
year = {},
note = {(Accessed on 01/19/2021)}
}

@misc{snell2017prototypical,
      title={Prototypical Networks for Few-shot Learning}, 
      author={Jake Snell and Kevin Swersky and Richard S. Zemel},
      year={2017},
      eprint={1703.05175},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}