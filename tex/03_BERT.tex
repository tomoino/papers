\documentclass{jsarticle}
\bibliographystyle{jplain}
\usepackage[dvipdfmx]{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{wrapfig}
\usepackage{MnSymbol}
\usepackage{scalefnt}
\usepackage{here}
\title{\vspace{-3cm}論文読みまとめ\\BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding}
\author{井上智裕}
\date{2021年1月6日}

\makeatletter
\renewcommand{\thefigure}{\thesection.\arabic{figure}}
\@addtoreset{figure}{section}
\renewcommand{\thetable}{\thesection.\arabic{table}}
\@addtoreset{table}{section}
\renewcommand{\theequation}{\thesection.\arabic{equation}}
\@addtoreset{equation}{section}

\newcommand{\argmax}{\mathop{\rm arg~max}\limits}

\begin{document}
\maketitle
\vspace{-1cm}
\section{導入}
\subsection{扱う問題}
言語モデルの事前学習（自然言語処理）

\subsection{問題意識}
自然言語処理タスクの改善には、言語モデルの事前学習が有効であることが示されている。
事前学習された言語モデルを下流のタスクに適用する方法として特徴量ベースのアプローチと
ファインチューニングによるアプローチがある。
特徴量ベースのアプローチは事前学習により得られた表現をタスク固有のアーキテクチャにおいて
特徴量の一部として扱う方法である。単語埋め込みベクトルの事前学習には
このアプローチがとられ、文の埋め込みや段落の埋め込みに対しても
その手法が一般化されてきた。
一方で、ファインチューニングは事前学習によって得られたパラメタを初期値として
タスクに対して学習させる方法である。
これらの方法では、共通して事前学習において一方向性の言語モデルを使用するため、
文レベルのタスク（文間の関係を予測するための自然言語推論や言い換えなど）や
質問応答のようなトークンレベルのタスクなど、前後両方向の文脈情報が重要であるタスクに
適さないという問題がある。

既存手法についてより具体的に言及する。
ELMoという手法は
左から右、右から左への言語表現を連結することで各トークンの表現を得る手法であるが、
これは教師なし特徴量ベースのアプローチであり、深い双方向性は持たない。
また、近年の手法として、
文脈を考慮して教師なしで事前学習した文または文書エンコーダーを
教師ありの下流タスクにファインチューニングするアプローチが取られてきた。
この手法ではゼロから学習するパラメタが少ないという利点があるが、
事前学習に使用される言語モデルは一方向性のものである。
OpenAI GPTではこの手法が取られており、以前に最先端の結果を多くの文レベルタスクで残している。
この他に、自然言語推論や機械翻訳など大規模なデータセットを用いた教師ありタスクでは、
転移学習が有効であることが示されている。

\section{理論}
この論文では、ファインチューニングによるアプローチの改善として
BERT (Bidirectional Encoder Representations from Transformers) を提案している。
BERTモデルは、ラベル付けされていないデータを用いて複数のタスクで事前学習が行われ、
下流タスクのラベル付きデータを用いてファインチューニングされる。
つまり、BERTは下流タスクが異なっていても統一されたアーキテクチャであるという特徴を持つ。

\begin{figure}[H]
  \begin{center}
    \includegraphics[clip,width=12.0cm]{image/03_BERT/fig1.PNG}
    \caption{BERTの事前学習とファインチューニング}
    \label{fig:fig1}
  \end{center}
\end{figure}

\subsection{モデル構造}
BERTのアーキテクチャは多層双方向 Transformer のエンコーダーである。
Transformerのブロック数をL、隠れ層のサイズをH、self-attention ヘッドの数をAとして、
この論文で提案されたモデルは
$\rm BERT_{BASE}$ (L=12, H=768, A=12, パラメタ総数=1.1億個)と
$\rm BERT_{LARGE}$ (L=24, H=1024, A=16, パラメタ総数=3.4億個)である。
モデルへの入力列は単一文の場合も複数の文の場合もあるため、最初の文頭を表す[CLS]トークンと
文同士を分離する[SEP]トークンが挿入される。
入力表現はトークン、セグメントおよび位置の埋め込み表現を加算されることで得られる。

\begin{figure}[H]
  \begin{center}
    \includegraphics[clip,width=12.0cm]{image/03_BERT/fig2.PNG}
    \caption{BERTの入力表現}
    \label{fig:fig2}
  \end{center}
\end{figure}


\subsection{事前学習}
BERTでは、従来の左から右への一方向の言語モデルを用いない。
代わりに
masked language model (MLM) と next sentence prediction (NSP) の
2つの教師なしをタスクを
事前学習に導入する。

MLMは、深い双方向表現の学習のために入力トークンの一部（この論文では15\%）を
ランダムにマスクして、そのマスクされたトークンを予測するタスクである。
これにより双方向の事前学習モデルを得ることができるが、
[MASK]トークンはファインチューニングの際には現れないため、
事前学習とのミスマッチが生じるという欠点がある。
これを軽減するために常に[MASK]トークンに置き換えるのではなく、
選択されたトークンのうち80\%を[MASK]トークンに置き換え、
10\%をランダムなトークンに置き換え、
残りの10\%を変更前のトークンに戻す。

NSPは、言語のモデル化では直接捉えることのできない
2つの文の関係性を考慮するモデルの構築のために行われる次文予測タスク
である。
具体的にはある文Aに対して、別のある文Bが実際に続く次の文かそうでないのかを
二値で予測するタスクである。


\subsection{ファインチューニング}
Transformer の self-attention 機構により、BERTは適切な入出力を入れ替えることが
できるため、ファインチューニングは容易である。
すなわち、ファインチューニングにおいては各タスク固有の入出力をBERTに与えるだけで良い。

\section{実験}
どんな実験設定か？どんな点で他の手法より良くなったか？

\subsection{GLUE}
General Language Understanding Evaluation (GLUE) は、
8つの自然言語理解タスクの集合体である。
32のバッチサイズを使用し、
すべてのGLUEタスクについて、データ上で3エポック分の
ファインチューニングを行った。
また、各タスクについて、
Devセット上で最適な学習率（5e-5, 4e-5, 3e-5, 2e-5のいずれか）
を選択した。
結果は図\ref{fig:tb1}のようになった。
$\rm BERT_{BASE}$および$\rm BERT_{LARGE}$ の両方がすべてのタスクにおいて
従来のシステムを大幅に上回る結果となった。

\begin{figure}[H]
  \begin{center}
    \includegraphics[clip,width=12.0cm]{image/03_BERT/tb1.PNG}
    \caption{GLUEの結果}
    \label{fig:tb1}
  \end{center}
\end{figure}


\subsection{SQuAD v1.1}
Stanford Question Answering Dataset (SQuAD v1.1)は、
クラウドソーシングされた質問と回答のペアの集合である。
質問とその答えを含むWikipediaの一節が与えられた場合、
答えに相当するテキストスパンを予測するタスクである。
学習率は5e-5、バッチサイズは32で、3エポックでファインチューニングを行った。
なお、SQuADでのファインチューニングに先立ち、最初にTriviaQAでファインチューニングを行った。
結果は図\ref{fig:tb2}のようになった。
F1スコアで、アンサンブルでは+1.5、シングルモデルでは+1.3
トップリーダーボードシステムを上回った。

\begin{figure}[H]
  \begin{center}
    \includegraphics[clip,width=12.0cm]{image/03_BERT/tb2.PNG}
    \caption{SQuAD v1.1の結果}
    \label{fig:tb2}
  \end{center}
\end{figure}

\subsection{SQuAD v2.0}
SQuAD 2.0タスクは、SQuAD 1.1に
段落に答えが存在しないという選択肢を追加したものである。
学習率5e-5、バッチサイズ48で2エポックのファインチューニングを行った。
結果は図\ref{fig:tb3}のようになった。
従来のSoTAモデルと比較して、F1スコアで+5.1の改善が見られた。

\begin{figure}[H]
  \begin{center}
    \includegraphics[clip,width=12.0cm]{image/03_BERT/tb3.PNG}
    \caption{SQuAD v2.0の結果}
    \label{fig:tb3}
  \end{center}
\end{figure}

\subsection{SWAG}
Situations With Adversarial Generations (SWAG)には、
根拠のある常識的な推論を行うタスクであり、
与えられた文に対して続く文として、
4つの選択肢の中から最も妥当なものを選択するタスクである。
学習率2e-5、バッチサイズ16の3エポックでモデルをファインチューニングした。
結果は図\ref{fig:tb4}のようになった。
$\rm BERT_{LARGE}$ はESIM+ELMoシステムを+27.1\%、OpenAI GPTを+8.3\%上回った。

\begin{figure}[H]
  \begin{center}
    \includegraphics[clip,width=12.0cm]{image/03_BERT/tb4.PNG}
    \caption{SWAGの結果}
    \label{fig:tb4}
  \end{center}
\end{figure}


\subsection{アブレーションスタディ：事前学習タスクによる影響}
BERTの深い双方向性の重要性を確かめるために事前学習の条件を変える実験を行った。
1つ目としてNSPタスクなしでMLMタスクのみを行うモデル、
2つ目としてNSPタスクなしでMLMタスクの代わりに標準的な LTR (Left-to-Right) 言語モデル
を用いて学習される左コンテクストのみのモデル、
3つ目としてLTRモデルにBiLSTMを加えたモデル
に対して、事前学習を行った。
結果は図\ref{fig:tb5}のようになった。
これより、NSPタスクの除外によって
QNLI、 MNLI、 SQuAD 1.1 において性能が著しく低下することが確認できる。
また、MLMによる双方向性表現の学習をなくすことで、全てのタスクにおいて性能は低下し、
MRPCとSQuADで大きな低下が見られた。
また、LTRモデルにBiLSTMを加えたモデルでは
SQuADでは改善が見られるが、GLUEでは性能が低下した。

\begin{figure}[H]
  \begin{center}
    \includegraphics[clip,width=12.0cm]{image/03_BERT/tb5.PNG}
    \caption{事前学習タスクによる影響}
    \label{fig:tb5}
  \end{center}
\end{figure}

\subsection{アブレーションスタディ：モデルサイズによる影響}
BERTモデルの構造のうち、
Transformerのブロック数L、隠れ層のサイズH、self-attention ヘッドの数Aの影響を調べる。
パラメタを変更したときのGLUEタスクでの結果は図\ref{fig:tb6}のようになった。
これより、下流タスクのデータが少ないMRPCのようなタスクに対しても、
大きなモデルほど高い精度を示すことが確認できる。
また、モデルサイズを大きくすることで、
機械翻訳や言語モデリングのような大規模なタスクだけではなく
小さなスケールのタスクにおいても改善が見られることが確認できた。

\begin{figure}[H]
  \begin{center}
    \includegraphics[clip,width=12.0cm]{image/03_BERT/tb6.PNG}
    \caption{モデルサイズによる影響}
    \label{fig:tb6}
  \end{center}
\end{figure}


\subsection{アブレーションスタディ：BERTを用いた特徴量ベースの手法}
ここまでの BERT の結果はすべて、
事前学習されたモデルに単純な分類層を追加し、
下流のタスクですべてのパラメータを共同で微調整するという
ファインチューニングアプローチを使用していた。
しかし、事前学習されたモデルから固定の特徴量を抽出する特徴量ベースのアプローチには、
一定の利点がある。
すなわち、第一に
すべてのタスクがTransformerエンコーダアーキテクチャで
簡単に表現できるわけではないため、そういった場合に対して
タスク固有のモデルアーキテクチャを追加することで対応できること、
第二に計算コストの高い表現計算を事前に行い、その表現を用いて計算コストの低い実験を
多く行うことで計算コストを低減できることである。
ここでは、固有表現抽出タスクであるCoNLL-2003 NERタスクに
BERTを適用して、2つのアプローチを比較する。
結果は図\ref{fig:tb7}のようになった。
特徴量ベースのアプローチでもっとも性能の良いものとファインチューニングアプローチを比較すると、
その差はわずかにF1スコアで0.3であり、BERTがファインチューニングと
特徴量ベースのアプローチの両方で有効であることが確認できる。

\begin{figure}[H]
  \begin{center}
    \includegraphics[clip,width=12.0cm]{image/03_BERT/tb7.PNG}
    \caption{BERTによるファインチューニングアプローチと特徴量ベースアプローチの比較}
    \label{fig:tb7}
  \end{center}
\end{figure}

\section{結論・展望}
近年、言語モデルにおいて転移学習が行われるようになり、
教師なしの事前学習が多くの言語理解システムにおいて不可欠であることが
実証されてきた。
特に、低リソースのタスクであっても、事前学習を行ったモデルを利用することで
深い一方向性アーキテクチャの恩恵を受けることを可能にしてきた。
この論文は、深い双方向性を持ったモデルにおいても
共通の事前学習を行ったモデルが幅広いタスクに
適用できることを示している。


%% \begin{table}[htbp]
%%   \begin{center}
%%     \caption{何かの表}
%%     \begin{tabular}{|c|c|c|c|c|} \hline
%%       ユーザー & 映画1 & 映画2 & 映画3 & 映画4 \\ \hline 
%%       A & 5 & 3 &   &  \\
%%       B &  & 2 &  & 5 \\
%%       C &  &  & 4 & 3 \\ 
%%       D & 4 &  & 3 & \\
%%       E & 4 & 3 &  & 2\\ \hline
%%     \end{tabular}
%%     \label{tab:tab1}
%%   \end{center}
%% \end{table}

%% \begin{figure}[htbp]
%%   \begin{center}
%%     \includegraphics[clip,width=12.0cm]{/path/to/hoge.png}
%%     \caption{何かの図}
%%     \label{fig:fig1}
%%   \end{center}
%% \end{figure}

% \begin{eqnarray}
%   score(h_t,\bar h_s) = \begin{cases}
%     h_t^T\bar h_s & dot \\
%     h_t^TW_a\bar h_s & general \\
%     v_a^T \tanh(W_a [h_t;\bar h_s]) & concat \\
%   \end{cases}
% \end{eqnarray}

\end{document}
